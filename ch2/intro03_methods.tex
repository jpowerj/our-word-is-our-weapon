The key computational tool used throughout, to map texts into geometric spaces based on their semantic content, is a contextual sentence embedding algorithm. In fact, there are a wide range of embedding algorithms for transforming various linguistic entities (e.g., words, phrases, or nodes within a dependency parse tree) into points within a high-dimensional geometric space. Although our unit of analysis herein is a \textit{document} (e.g., a text, pamphlet, or letter), these documents are in general too long for a single embedding algorithm to handle -- state-of-the-art embedding algorithms like BERT have an upper limit of 512 tokens that can be jointly encoded into a single high-dimensional vector. Thus we instead compute a separate embedding for each \textit{sentence} within a text using SentenceTransformers \parencite{reimers_sentence-bert_2019}, then combine these sentence vectors into a single document vector via mean pooling. As a robustness check, however, we computed document-level embeddings via an experimental document embedding method called Longformer, described in \cite{beltagy_longformer_2020}, and obtained qualitatively similar results (see Appendix \ref{app:longformer}).

Turning to the issue of \textit{how} exactly the semantic information in a text is given a geometric interpretation: at the most basic level, sentence embedding algorithms take every word appearing in a corpus and represent them as points within a geometric space, such that words which are used in similar contexts\footnote{Although ``context'' can be operationalized in different ways based on what information a user hopes to extract, in our case the context of a word $w$ in a sentence $S$ is defined to be the set of $n$ words appearing before and after $w$ in the sentence. For example, if $S$ is ``\texttt{The sleepy grey cat likes salmon.}'', and $w$ is ``\texttt{cat}'', then the context of $w$ with $n = 2$ would be the set $\{\texttt{sleepy}, \texttt{grey}, \texttt{likes}, \texttt{salmon}\}$.} will be placed closer together in the space than sentences which use dissimilar words and/or dissimilar contexts around these words. Since the publication of the first widely-used word embedding algorithm, \texttt{word2vec} (\cite{mikolov_distributed_2013}), in 2012, researchers in the social sciences have used these algorithms to incorporate information from textual corpora into studies which previously were restricted to using numeric or qualitative data. Recent studies have found, for example, that contextual embeddings are able to capture salient properties of social class \parencite{kozlowski_geometry_2019}, the ideology of political manifestos \parencite{rheault_word_2020}, and the influence of economics on legal decisions \parencite{ash_ideas_2017}. Of these three, the latter comes closest to our work, in attempting to study the linguistic properties captured by word embeddings using econometric methods for estimation of time-series effects. A related literature, which predates the creation of word embedding algorithms, aims to quantitatively capture the existence, direction, and magnitude of ideological influence directly. \cite{barron_individuals_2018}, for example, studies ideological influence across a time series of French Revolutionary debate transcripts by introducing ``transience'' and ``novelty'' metrics, which quantify how much the content of a given text is adopted by future texts, and how much it differs from the content of earlier texts, respectively. Unlike the previously-mentioned studies, however, this literature has yet to incorporate newer embedding methods, instead opting for a set of older more established text-analysis methods known as Topic Models\footnote{Topic modelling algorithms such as Latent Dirichlet Allocation (LDA), the most widely-used method, utilize word collocation information to identify a set of $K$ semantic topics within a text corpus, where each topic is itself a probability distribution over words. When given a corpus of unlabeled New York Times articles, for example, these algorithms are able to identify a topic for which ``stock'', ``market'', and ``percent'' are the three highest-probability terms, another where ``restaurant'', ``sauce'', ``menu'' have highest probability, and so on \parencite{blei_topic_2013}. Thus, despite not having any information on what sections the articles were pulled from, the algorithm is able to group articles from the same section together with near-perfect accuracy.}. Although topic modelling algorithms are an effective tool for summarizing a corpus at a high level, they are ill-suited for the task of tracing out the trajectory of \textit{particular} terms or concepts over time. Since we are interested in how Marx was able to cement his particular set of terms as \textit{the} vocabulary for later socialist discourse, embeddings are uniquely effective in allowing us to look at exactly which contexts a given term was employed in by Marx, and how this differs from the term's typical contexts before and after Marx's intervention. In other words, while topic-modelling-based approaches can tell us \textit{that} Marx's writings influenced future socialist discourse, embedding-based approaches can tell us \textit{how} this influence operated -- which terms or concepts Marx utilized in particularly novel ways, and which of these illocutionary moves were and were not effective in terms of influencing subsequent socialist thought.

Studies across both of these literatures, moreover, have yet to adopt the contextual \textit{sentence} embeddings we use herein, which utilize a more recently-developed embedding method from 2019 known as \BERT{} \parencite{devlin_bert_2019}. \BERT{} constructs numeric representations for both words \textit{and} the contexts in which they appear, rather than associating each word with a single embedding vector. This means that, e.g., the word ``\texttt{bank}'' within the sentence ``\texttt{I took my money to the bank.}'' would be given a different embedding from the same word within the sentence ``\texttt{I took a nap on the bank of the river.}''. This improvement upon the original set of word embedding algorithms is crucial, we contend, for capturing the nuanced uses of language which occur frequently in political-ideological polemics. Marx's 1860 polemic against Karl Vogt \parencite{marx_herr_1860}, for example, makes use of several puns and purposeful misspellings of the names of those he is attacking (for example, calling a particular political opponent with the surname Ranigel ``Ran-Igel'', likening him to an ``Igel'', the German word for hedgehog -- thus we would want to keep Marx's use of ``Igel'' in this sense separate from uses of ``Igel'' in general German texts). In general, due to the harsh censorship of political writings under the regimes of Friedrich Wilhelm III and IV (1797--1840 and 1840--1861, respectively)\footnote{See \cite{rose_reading_1978} for a detailed treatment of the effects of this censorship on Marx's writings, and \cite{prawer_karl_1976} for a chronological examination of the explicit and implicit literary references in Marx's writings.}, text-analysis methods which are unable to capture the wide variety of ironic, metaphorical, and figurative uses of words would thus be unable to capture many important political illocutions throughout 19th century German political discourse, which had to be performed almost exclusively by way of these figures of speech.

As discussed in more detail in Section \ref{sec:methods}, to further pinpoint the pathways of influence we employ a recently-developed modification of the original BERT algorithm which explicitly constructs a personalized embedding for each (word, author) pairing, in addition to the contextual embedding for each word relative to the entire corpus \parencite{welch_exploring_2020}. This enables us to capture not only the shared terms and figures of speech available to all authors within a discursive community, but also the \textit{particular} ways in which they are employed by individual authors. Thus, while studies like \cite{kozlowski_geometry_2019} are able to discover shifts in the overall discourse around social class during the 20th century, for example that different education-related terms tended to become more dichotomous along the upper-class/lower-class axis, this modification of BERT would allow us to identify which authors in particular were ahead of the curve, adopting the newer senses of education-related terms earlier than others. Although testing for the causal influence of these early-adopters on the broader discourse is outside the scope of this paper, we are still therefore able to identify a necessary condition for causal influence---temporal precedence---and thus develop plausible causal hypotheses based on which authors' embeddings precede the overall embeddings in terms of the movement of key terms along socially meaningful axes. In particular, in Section \ref{sec:results} we will identify a movement in the language of 19th century socialist discourse writ large, away from philosophically-framed Hegelian terms and towards a more positivistic political-economic vocabulary, then identify Marx as a plausible cause of this shift due to the correlation between Marx's trajectory along this axis to the lagged trajectory of the corpus of 19th century socialist texts along the same axis.
